{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "\n",
    "# true value = 추정과 학습의 대상\n",
    "\n",
    "true_b = 1 # true bias\n",
    "true_w = 2 # true weight\n",
    "N = 100 # number of data\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N,1) # runif(N), (N,1) arrary return\n",
    "epsilon = 0.1 * np.random.randn(N,1) # rnorm(N)\n",
    "y = true_b + true_w * x + epsilon # vector 곱 ( => @ : 행렬곱)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation Split\n",
    "\n",
    "# validation과 test 구분할 것\n",
    "# train과 test의 분포가 서로 같아야함!!!! => need to be checked\n",
    "\n",
    "# shuffles the indices\n",
    "idx = np.arange(N) # 0~99 vector 생성\n",
    "split_index = int(N * 0.8) # int() 정수로 반환\n",
    "\n",
    "# split_index 전/후로 Train-Validation split\n",
    "train_idx = idx[:split_index] # 0~79\n",
    "val_idx = idx[split_index:] # 80~99\n",
    "\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our goal : validation set을 가장 잘 설명하는 w와 b를 train set을 통해 도출\n",
    "# y와 y_hat의 차이가 가장 작아지도록 : minimize MSE(loss)\n",
    "\n",
    "# loss surface : w_hat, b_hat을 grid로 하고 그 때의 MSE\n",
    "# gradient descent : 미분값을 빼주면 방향성 설정 가능\n",
    "# gradient = 최소점으로 가는 벡터의 방향성을 의미함\n",
    "# 실제 paramater : gradient * learning rate로 업데이트 => 수렴을 위하여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 측정하는 코드\n",
    "# @timer를 함수 위에 달아둘 것!!\n",
    "import time\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args,**kwargs)\n",
    "        end_time = time.time()\n",
    "        computation_time = end_time - start_time\n",
    "        print(f\"Execution time of {func.__name__}: {computation_time} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of train_model_numpy: 0.026873350143432617 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.02341381]), array([1.93680715]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient Descent for Linear regression\n",
    "@timer\n",
    "def train_model_numpy(lr = 0.1, epochs = 1000):\n",
    "    # Initialize parameters : DL의 성능에 큰 영향\n",
    "    b = np.random.randn(1)\n",
    "    w = np.random.randn(1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Loss computation\n",
    "        y_hat = b + w * x_train\n",
    "        error = (y_hat - y_train)\n",
    "        mse_loss = np.mean(error ** 2)\n",
    "        \n",
    "        # Gradient computation : 미분 수식 넣어볼것\n",
    "        b_grad = 2 * np.mean(error)\n",
    "        w_grad = 2 * np.mean(x_train * error)\n",
    "        b = b - lr * b_grad\n",
    "        w = w - lr * w_grad\n",
    "    return b, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of train_model_numpy: 0.023286819458007812 seconds\n",
      "b_estimate:[1.02341396], w_estimate[1.93680685]\n"
     ]
    }
   ],
   "source": [
    "b_hat, w_hat = train_model_numpy()\n",
    "print(\"b_estimate:{}, w_estimate{}\".format(b_hat, w_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch\n",
    "\n",
    "import torch\n",
    "\n",
    "# create tensor at CPU\n",
    "x_train_tensor = torch.as_tensor(x_train)\n",
    "y_train_tensor = torch.as_tensor(y_train)\n",
    "\n",
    "# create tensor at GPU\n",
    "# 현재 GPU가 연결이 되어있다면 그곳에 자료를 넣겠다\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "x_train_tensor = torch.as_tensor(x_train).to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).to(device)\n",
    "\n",
    "x_val_tensor = torch.as_tensor(x_val).to(device)\n",
    "y_val_tensor = torch.as_tensor(y_val).to(device)\n",
    "\n",
    "# CPU,GPU 상의 data는 통신 없이는 서로 계산이 불가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of train_model_torch: 0.1213986873626709 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.0234], requires_grad=True), tensor([1.9368], requires_grad=True))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Descent by PyTorch\n",
    "@timer\n",
    "def train_model_torch(lr = 0.1, epochs = 1000):\n",
    "\n",
    "    # Initialize paramaters\n",
    "    \n",
    "    # requires_grad = True : 학습대상임을 설정\n",
    "    # dtype : 수치형 자료의 타입을 지정(default = float32)\n",
    "    # device : CPU인지 GPU인지\n",
    "    b = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
    "    w = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Loss computation\n",
    "        y_hat = b + w * x_train_tensor\n",
    "        error = (y_hat - y_train_tensor)\n",
    "        mse_loss = torch.mean(error ** 2)\n",
    "\n",
    "        # Gradient computation and descent\n",
    "\n",
    "        # backward : gradient를 자동으로 계산해 b,w안에 저장해줌\n",
    "        mse_loss.backward()\n",
    "\n",
    "        # update는 직접 해줘야함\n",
    "        # in-place operation : memory의 위치가 변하지 않음(같은 메모리 주소에 값을 업데이트)\n",
    "        # 만약 memory 주소가 바뀐다면 b.grad에 저장된 정보들을 불러올 때 큰 computation loss가 발생\n",
    "        # no_grad() : autograd를 멈춤\n",
    "        with torch.no_grad():\n",
    "            b -= lr * b.grad \n",
    "            w -= lr * w.grad\n",
    "        b.grad.zero_() # zero_ : in-place operation\n",
    "        w.grad.zero_()\n",
    "    return b, w\n",
    "\n",
    "train_model_torch()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of train_model_torch_optim: 0.13792657852172852 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.0234], requires_grad=True),\n",
       " tensor([1.9368], requires_grad=True),\n",
       " tensor(0.0098, dtype=torch.float64, grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "'''\n",
    "learning rate : 미분값을 얼마나 이동시킬 것인지(계수) \n",
    "lr이 크다면 초반엔 loss가 빠르게 줄어들지만 후반부에 underfitting이 발생할 수 있음\n",
    "'''\n",
    "@timer\n",
    "def train_model_torch_optim(lr = 0.1, epochs = 1000):\n",
    "\n",
    "    # Initialize paramaters\n",
    "    \n",
    "    # requires_grad = True : 학습대상임을 설정\n",
    "    # dtype : 수치형 자료의 타입을 지정(default = float32)\n",
    "    # device : CPU인지 GPU인지\n",
    "    b = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
    "    w = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
    "    \n",
    "    parameters = [b, w]\n",
    "    optimizer = optim.SGD(parameters, lr = lr) # optimizer의 종류, 최적화대상, learning rate 지정\n",
    "    mse_loss = nn.MSELoss() # 다른 종류의 loss를 사용하고 싶은 경우 함수만 바꿔주면됨\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Loss computation\n",
    "        y_hat = b + w * x_train_tensor\n",
    "        loss = mse_loss(y_hat, y_train_tensor) \n",
    "\n",
    "        # Gradient computation and descent\n",
    "        loss.backward()\n",
    "        optimizer.step() # paramater update\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    y_pred = b + w * x_val_tensor\n",
    "    test_MSE = mse_loss(y_pred, y_val_tensor)\n",
    "\n",
    "    return b, w, test_MSE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of train_model_torch_optim: 0.13974785804748535 seconds\n",
      "b_estimate:tensor([1.0234], requires_grad=True), w_estimatetensor([1.9368], requires_grad=True), test_MSE0.009755489943201115\n"
     ]
    }
   ],
   "source": [
    "b_hat, w_hat, test_MSE = train_model_torch_optim()\n",
    "print(\"b_estimate:{}, w_estimate{}, test_MSE{}\".format(b_hat, w_hat, test_MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('C:/Users/hecor/Downloads/quiz_data.pkl', 'rb') as f:\n",
    "  data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict 형태의 data(key를 이용하여 불러와야함)\n",
    "N = len(data['x'])\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "\n",
    "idx = np.arange(N) # 0~99 vector 생성\n",
    "split_index = int(N * 0.8) # int() 정수로 반환\n",
    "\n",
    "# split_index 전/후로 Train-Validation split\n",
    "train_idx = idx[:split_index] # 0~79\n",
    "val_idx = idx[split_index:] # 80~99\n",
    "\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]\n",
    "\n",
    "# GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "x_train_tensor = torch.as_tensor(x_train).to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).to(device)\n",
    "\n",
    "x_val_tensor = torch.as_tensor(x_val).to(device)\n",
    "y_val_tensor = torch.as_tensor(y_val).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of train_model_torch_optim: 0.1363527774810791 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-0.3087], requires_grad=True),\n",
       " tensor([-0.1602], requires_grad=True),\n",
       " tensor(0.2981, dtype=torch.float64, grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
